Acknowledgements to Liu et al.(https://arxiv.org/abs/2304.08485) for the code to train multimodal models. The r1 distilled qwen 1.5 b vision model is here (INSERT HUGGINGFACE LINK). This model was finetuned on LLaVA-Instruct-158K. There is one pretrained mm_projector in the checkpoints folder, trained on  CC-595K dataset. The vision encoder used was CLIP from OpenAI https://arxiv.org/pdf/2103.00020.
