Acknowledgements to Liu et al.(https://arxiv.org/abs/2304.08485) for the code to train multimodal models. The r1 distilled qwen 1.5 b vision model is here https://huggingface.co/matboz/R1_distill_qwen_1.5_vision. This model was finetuned on LLaVA-Instruct-158K. There is one pretrained mm_projector in the checkpoints folder, trained on  CC-595K dataset. The vision encoder used was CLIP from OpenAI https://arxiv.org/pdf/2103.00020. To generate responses use the model_generation.ipynb file. To train a model use the untitled.ipynb file and go to the pretrain.sh/finetune.sh files and replace the placeholder files with your files.
